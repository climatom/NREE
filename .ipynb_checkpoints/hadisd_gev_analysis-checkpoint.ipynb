{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a38de41",
   "metadata": {},
   "source": [
    "# Generalized Extreme Value (GEV) analysis of HadISD weather station data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7aede4",
   "metadata": {},
   "source": [
    "This dataframe is organized into four parts. The first part imports all necessary packages and data. The data is then cleaned and prepared for analysis. The second part investigates which station data are appropriate for GEV analysis using Kolmogorov-Smirnov (KS) testing. The third part fits non-stationary GEV models to the remaining data, and projects future heat stress. The fourth and final part visualizes and exports results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faed47d",
   "metadata": {},
   "source": [
    "## Part I: Data preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Import HasISD extremes dataset:\n",
    "\n",
    "# Import necessary libraries: \n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import climextremes \n",
    "from scipy.stats import genextreme as gev\n",
    "from scipy.stats import kstest\n",
    "from scipy.interpolate import barycentric_interpolate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from global_land_mask import globe\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Read HadISD extremes dataset and convert to dataframe.\n",
    "fn = \"./hadisd_d2_distances_daily_max_wsid.nc\"\n",
    "ds = xr.open_dataset(fn)\n",
    "df = ds.to_dataframe()\n",
    "df = df.rename(columns={'lh_distance_top6_daily_maxima': 'maxima'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302afec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Clean data in preparation for analysis. Current station count: 9555. \n",
    "\n",
    "# Convert 6 hour annual maxima observations into average max observation:\n",
    "df_avg = df.groupby(['station', 'year']).agg({'maxima': 'mean', 'SID': 'first', \"lon\": \"first\", \"lat\": \"first\"})\n",
    "\n",
    "# Drop all years prior to 1970:\n",
    "df_avg = df_avg[df_avg.index.get_level_values('year') >= 1970]\n",
    "\n",
    "# Convert SID column so it contains only the last number after the second underscore:\n",
    "# Split the column into multiple columns based on the underscore separator\n",
    "split_df = df_avg['SID'].str.split('_', expand=True)\n",
    "\n",
    "# Select the third column, which contains the value after the second underscore\n",
    "new_col = split_df.iloc[:, 2]\n",
    "\n",
    "# Replace the original column with the new column\n",
    "df_avg['SID'] = new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Add relevant running 30-year global average temperature to each datapoint:\n",
    "\n",
    "# Read NASA GISTEMP data:\n",
    "url = \"https://data.giss.nasa.gov/gistemp/graphs_v4/graph_data/Global_Mean_Estimates_based_on_Land_and_Ocean_Data/graph.csv\"\n",
    "global_temp = pd.read_csv(url)\n",
    "global_temp.reset_index(inplace=True)\n",
    "global_temp.rename(columns = global_temp.iloc[0], inplace = True) #rename columns using first row of data\n",
    "global_temp = global_temp.iloc[1:] #cut duplicate first row of data\n",
    "global_temp = global_temp.drop(\"Lowess(5)\",axis=1)\n",
    "global_temp['Year'] = pd.to_numeric(global_temp['Year'], errors='coerce').astype('Int64')\n",
    "global_temp['No_Smoothing'] = pd.to_numeric(global_temp['No_Smoothing'], errors='coerce').astype('float64')\n",
    "\n",
    "# Adjust to 1880-1910 period instead of 1950-1980, as used by GISTEMP: \n",
    "GT_1880_1910 = global_temp.loc[(global_temp['Year'] >= 1880) & (global_temp['Year'] <= 1910)]\n",
    "average_temp = GT_1880_1910['No_Smoothing'].mean()\n",
    "global_temp['Adjusted_Temp'] = global_temp['No_Smoothing'] - average_temp \n",
    "global_temp = global_temp.drop('No_Smoothing', axis=1)\n",
    "\n",
    "# Add a running 30-year global average to the data:\n",
    "global_temp[\"temp_30yr_avg\"] = global_temp[\"Adjusted_Temp\"].rolling(window=30).mean()\n",
    "\n",
    "# Add the relevant global average temperature data into the HasISD data:\n",
    "year_temp_map = global_temp.set_index('Year')['temp_30yr_avg'].to_dict()\n",
    "df_avg['Glob_Av_Temp'] = df_avg.index.get_level_values('year')\n",
    "df_avg['Glob_Av_Temp'] = df_avg['Glob_Av_Temp'].map(year_temp_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385bc51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Mask out all of the ocean stations, and then all stations above 60N and below 60S:\n",
    "\n",
    "from global_land_mask import globe\n",
    "import numpy as np\n",
    "\n",
    "# Check if a point is on land:\n",
    "is_on_land = globe.is_land(df_avg['lat'], df_avg['lon'])\n",
    "df_avg['land_mask'] = is_on_land\n",
    "\n",
    "# Remove ocean and lat above 60N or below 60S:\n",
    "df_analysis = df_avg.loc[(df_avg['land_mask'] == 1) & (df_avg['lat'].between(-60, 60))]\n",
    "df_analysis = df_analysis.drop(\"land_mask\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b318fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 5: Clean data to remove any remaining stations which exhibit particularly weird behavior not caught by\n",
    "### the HadISD QC protocols:\n",
    "\n",
    "# Remove all stations with less than 50% of data available between 1970 and 2020:\n",
    "\n",
    "# Calculate the percentage of NaN values in the \"Maxima\" column for each station\n",
    "percent_nan = df_analysis.groupby('station')['maxima'].apply(lambda x: x.isna().mean())\n",
    "\n",
    "# Select only the stations where the percentage of NaN values is less than or equal to 50%\n",
    "selected_stations = percent_nan[percent_nan <= 0.5].index\n",
    "\n",
    "# Filter the original DataFrame to keep only the selected stations\n",
    "df_analysis = df_analysis.loc[selected_stations]\n",
    "\n",
    "# Create a list of locs that have their largest maxima before the year 1990:\n",
    "\n",
    "# create a boolean mask to identify rows to keep\n",
    "mask = df_analysis.groupby('station')['maxima'].transform(lambda x: x.idxmax()[1] >= 1990)\n",
    "\n",
    "# filter dataframe using the mask\n",
    "df_analysis = df_analysis[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daa5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data inhomogeneity and remove any stations with insufficient homogeniety values:\n",
    "\n",
    "inhom_data = pd.read_csv('https://www.metoffice.gov.uk/hadobs/hadisd/v330_2022f/files/MergedBreaks_temperatures_hadisd.dat')\n",
    "inhom_data = inhom_data.drop('Delete', axis=1)\n",
    "inhom_data = inhom_data.rename(columns={'Stat_ID': 'SID'})\n",
    "\n",
    "# Split the Break Date into multiple columns based on the dash separator\n",
    "split_df = inhom_data['Break_Date'].str.split('-', expand=True)\n",
    "\n",
    "# Select the first column, which contains the year\n",
    "new_col = split_df.iloc[:, 0]\n",
    "\n",
    "# Replace the original column with the new column\n",
    "inhom_data['Break_Date'] = new_col\n",
    "inhom_data['Break_Date'] = inhom_data['Break_Date'].astype(int)\n",
    "inhom_data = inhom_data[inhom_data['Break_Date'] >= 1970]\n",
    "\n",
    "# create a boolean mask to filter the DataFrame for inhomogeneities less than 1C\n",
    "mask = (inhom_data['Temp_Err'] > -1) & (inhom_data['Temp_Err'] < 1) | (inhom_data['Temp_Err'] == -99.99)\n",
    "\n",
    "# apply the mask to filter the DataFrame\n",
    "inhom_data_ref = inhom_data[mask]\n",
    "\n",
    "# remove any stations that are not in the inhom_data_ref list:\n",
    "\n",
    "# get the set of SIDs that occur in both dataframes\n",
    "common_sids = set(df_analysis['SID']).intersection(set(inhom_data_ref['SID']))\n",
    "\n",
    "# filter df1 to include only rows where the SID column is in the set of common SIDs\n",
    "df_analysis = df_analysis[df_analysis['SID'].isin(common_sids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc0999",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 6: Fill missing data prior to analysis, as KS check and GEV fit cannot be done with NaN values:\n",
    "\n",
    "# Split dataframe into stations with large gaps (greater than 5 year consecutive missing), and small gaps:\n",
    "\n",
    "# group by station and find consecutive NaN values\n",
    "groups = df_analysis.groupby('station',group_keys=False)['maxima'].apply(lambda x: x.isna().rolling(window=5, min_periods=1).sum())\n",
    "\n",
    "# get the unique stations with at least 5 consecutive NaN values\n",
    "nan_stations = groups[groups >= 5].reset_index()['station'].unique()\n",
    "\n",
    "# create two dataframes based on the condition\n",
    "df1 = df_analysis[df_analysis.index.get_level_values('station').isin(nan_stations)]\n",
    "df2 = df_analysis[~df_analysis.index.get_level_values('station').isin(nan_stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e601168",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for small groups of missing data, interpolate:\n",
    "\n",
    "# Group df2 by the first index level\n",
    "grouped = df2.groupby(level=0)\n",
    "\n",
    "# Loop over each group\n",
    "for group_name, group_data in grouped:\n",
    "    \n",
    "    station_data = group_data.copy().reset_index(level='station', drop=True)\n",
    "\n",
    "    # Check if there are any NaN values in the 'maxima' column\n",
    "    if station_data['maxima'].isna().any():\n",
    "    \n",
    "        # Get the non-NaN values and their corresponding indices\n",
    "        x = station_data.index[~station_data['maxima'].isna()].values\n",
    "        y = station_data.loc[~station_data['maxima'].isna(), 'maxima'].values\n",
    "\n",
    "        # Get the indices of the rows where 'maxima' is NaN\n",
    "        nan_indices = station_data.index[station_data['maxima'].isna()].values\n",
    "     \n",
    "        # Perform interpolation for NaN values:\n",
    "        interpolated_values = np.interp(nan_indices, x, y)\n",
    "    \n",
    "        # For station value in dataframe, replace NaN values in maxima with interpolated_values in order:\n",
    "        station_data.loc[station_data['maxima'].isna(), 'maxima'] = interpolated_values\n",
    "\n",
    "    # Write the updated station_data back to the original dataframe\n",
    "    df2.loc[group_name, station_data.columns] = station_data.values\n",
    "\n",
    "# Combine df1 and the now-complete df2 so that all data is available for triangulation:\n",
    "df_combined = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e8c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For large groups of missing data, regress against three closest stations with complete data using multi-\n",
    "### variate regression. \n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++ Step 1 +++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "## Create new dataframe which has just station list and unique lat and lon values. This will then be used \n",
    "## to calculate distances and determine which stations to regress to eachother and which to eliminate.\n",
    "\n",
    "# group by station and aggregate lat and lon\n",
    "unique_stations = df_combined.groupby(level=0).agg({\"lat\": \"first\", \"lon\": \"first\"})\n",
    "\n",
    "# define haversine formula:\n",
    "\n",
    "import math\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "  \n",
    "    # Convert degrees to radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "\n",
    "    # Radius of the Earth in kilometers\n",
    "    earth_radius = 6371.0\n",
    "\n",
    "    # Calculate the differences in latitude and longitude\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Calculate the Haversine formula\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    distance = earth_radius * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++ Step 2 +++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "## Set up for loop to loop through all the unique stations in df_combined: \n",
    "\n",
    "for station_value in nan_stations:\n",
    "\n",
    "    ## Calculate the distance from a given station for all other stations:\n",
    "\n",
    "    first_row = unique_stations.xs(station_value)\n",
    "    unique_stations['Distance'] = unique_stations.apply(lambda row: calculate_distance(first_row['lat'], first_row['lon'], row['lat'], row['lon']), axis=1)\n",
    "    unique_stations['NaN_Count'] = unique_stations.index.isin(nan_stations).astype(int)\n",
    "    unique_stations = unique_stations.sort_values(by='Distance')\n",
    "\n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++ Step 3 +++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    ## Make a new dataframe from df_combined that has just maxima values for the stations of interest:\n",
    "\n",
    "    # Filter dataframe based on conditions\n",
    "    filtered_df = unique_stations[(unique_stations['NaN_Count'] == 0) & (unique_stations['Distance'] < 150)]\n",
    "\n",
    "    # Retrieve the first index value and the next four values (if available). If not remove station from\n",
    "    # df_combined dataframe:\n",
    "\n",
    "    if len(filtered_df) >= 5:\n",
    "        index_list = [unique_stations.index[0]] + list(filtered_df.index[1:4])\n",
    "\n",
    "        # Reset the index to make 'station' and 'year' as regular columns\n",
    "        placeholder = df_combined.copy()\n",
    "        placeholder.reset_index(inplace=True)\n",
    "\n",
    "        # Create the new dataframe\n",
    "        new_df = placeholder.pivot(index='year', columns='station', values='maxima')\n",
    "        new_df = new_df.reindex(columns=index_list)\n",
    "    \n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++ Step 4 +++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        # Fill NaN with multivariate regression:\n",
    "\n",
    "        target_column = new_df.columns[0]\n",
    "        train_data = new_df.dropna(subset=[target_column])\n",
    "\n",
    "        predictor_cols = train_data.columns[1:]  # Assumes the predictor columns start from the second column\n",
    "        X_train = train_data[predictor_cols]\n",
    "        y_train = train_data[target_column]\n",
    "    \n",
    "        regression_model = LinearRegression()\n",
    "        regression_model.fit(X_train, y_train)\n",
    "\n",
    "        nan_data = new_df[new_df[target_column].isna()]\n",
    "        X_nan = nan_data[predictor_cols]\n",
    "        predicted_values = regression_model.predict(X_nan)\n",
    "\n",
    "        new_df.loc[new_df[target_column].isna(), target_column] = predicted_values\n",
    "\n",
    "        idx = pd.IndexSlice\n",
    "        top_level_value = new_df.columns[0]\n",
    "        df_combined.loc[idx[top_level_value, :], 'maxima'] = new_df.iloc[:, 0].values\n",
    "\n",
    "    else:\n",
    "        first_value = unique_stations.index[0]\n",
    "        df_combined = df_combined.drop(index=first_value, level='station')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eaefbe",
   "metadata": {},
   "source": [
    "## Part II: Data KS viability check: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc61fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Fit a stationary GEV function to each location in the dataframe. Conduct a KS test to determine if \n",
    "### the data is appropriately represented by the best-fit function. Append the p-values to the df_analysis \n",
    "### dataframe. \n",
    "\n",
    "from scipy.stats import genextreme, kstest\n",
    "\n",
    "# Initialize an empty dictionary to store the p-values\n",
    "p_values = {}\n",
    "\n",
    "# Get the unique locations\n",
    "unique_locations = df_analysis.index.get_level_values(0).unique()\n",
    "\n",
    "# Loop through the unique locations\n",
    "for location in unique_locations:\n",
    "    # Get the data for the current location\n",
    "    location_data = df_analysis.loc[location]['maxima'].copy()\n",
    "    # Fit a GEV distribution to the data\n",
    "    params = genextreme.fit(location_data)\n",
    "    # Perform the KS test\n",
    "    D, p = kstest(location_data, params)\n",
    "    # Store the p-value in the dictionary\n",
    "    p_values[location] = p\n",
    "\n",
    "# Add the p-values to the dataframe\n",
    "df_analysis['p_value'] = df_analysis.index.get_level_values(0).map(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: remove all p-values smaller than 0.05:\n",
    "master = df_analysis[df_analysis['p_value'] > 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea9368",
   "metadata": {},
   "source": [
    "## Part III: Non-stationary GEV fit log-likelihood ratio test:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2340e72",
   "metadata": {},
   "source": [
    "Because climextremes doesn't have a log-likelihood function  built in, we need to export the master\n",
    "dataframe and ingest it into R. Using the gev.fit function, we will conduct both stationary and non-stationary\n",
    "GEV fits for the remaining weather stations, and conduct a log-likelihood ratio test to determine the \n",
    "weather stations for which non-stationary models provide a better fidelity fit. We will then re-ingest the \n",
    "master dataframe and continue to Part IV, where we will use climextremes to calculate future return periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Export dataset\n",
    "master.to_csv('master_to_R.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e915bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: R code is here - do not run in python ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "master <- read.csv(\"./master_to_R.csv\")\n",
    "\n",
    "crit<-qchisq(.95, df=1) # Chi-squared threshold - to reject null hypothesis (no diff between stationary/non)\n",
    "LL_test <- c()\n",
    "unique_locs <- unique(master$loc)\n",
    "\n",
    "for (i in unique_locs) {\n",
    "  maxima_values <- master %>% filter(loc == i) %>% pull(maxima)\n",
    "  glob_av_temp_values <- master %>% filter(loc == i) %>% pull(Glob_Av_Temp)\n",
    "  glob_av_temp_matrix <- matrix(glob_av_temp_values, nrow = length(maxima_values), ncol = 1)\n",
    "  fit_stat<-gev.fit(maxima_values)\n",
    "  fit_mov<-gev.fit(maxima_values, glob_av_temp_matrix, mul=1)\n",
    "  devi<-2*(fit_stat$nllh-fit_mov$nllh)\n",
    "  if(devi>crit){\n",
    "      LL_test <- c(LL_test, 1)\n",
    "  }else{\n",
    "      LL_test <- c(LL_test, 0)\n",
    "  }\n",
    "}\n",
    "\n",
    "master$LL_test <- LL_test[match(master$loc, unique_locs)]\n",
    "write.csv(master, \"./master_to_PY.csv\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b292496",
   "metadata": {},
   "source": [
    "## Part IV: Calculating future return periods with non-stationary GEV models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b424629",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Import GEV distribution data from R:\n",
    "master = pd.read_csv(\"./master_to_PY.csv\")\n",
    "master = master.set_index(['station','LL_test'])\n",
    "df_new = master[master.index.get_level_values(1) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96aa2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Calculate return periods for future warming states using GEV distribution data. Note this produces \n",
    "### NaNs for 84 of the 1412 stations which are removed from the analysis as a result. \n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "rows_list = []\n",
    "loc_list = []\n",
    "\n",
    "# Create a reference dataframe to contain the results\n",
    "df_plot_results = df_new.groupby(level = 'station').first()\n",
    "df_plot_results = df_plot_results[['lat', 'lon']]\n",
    "\n",
    "# Get the unique locations\n",
    "unique_locations = df_new.index.get_level_values(0).unique()\n",
    "\n",
    "# Loop through the unique locations\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"^R\\[write to console\\]:\")\n",
    "\n",
    "for location in unique_locations:\n",
    "    # Run non-stationary GEV fit projections for lethal heat at 1, 1.5, 2, 2.5, 3, and 3.5C global temp:\n",
    "    fit_ns = climextremes.fit_gev(np.array(df_new.loc[location]['maxima']), np.array(df_new.loc[location]['Glob_Av_Temp']),\n",
    "                                  locationFun = 1, getParams = True, xNew = np.arange(1, 4, 0.5), returnValue = 0)\n",
    "    # Convert log exceedence probabilities: \n",
    "    try:\n",
    "        result = np.exp(fit_ns['logReturnProb'])\n",
    "        result = np.nan_to_num(result)  # Replace NaN with zeros\n",
    "        # Append the results to rows:\n",
    "        rows_list.append(result)\n",
    "        loc_list.append(location)\n",
    "    except KeyError:\n",
    "        # Handle exception when object is not found\n",
    "        print(\"Object not found. Skipping append step.\")\n",
    "        continue\n",
    "\n",
    "# Create a dataframe out of the results, before joining with the location and lat/lon identifiers:\n",
    "columns = ['1C','1.5C','2C','2.5C','3C','3.5C']\n",
    "holding = pd.DataFrame(rows_list, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dcb454",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Create plotting dataframe:\n",
    "\n",
    "holding['loc'] = loc_list\n",
    "holding = holding.set_index('loc')\n",
    "plot_data = pd.concat([holding, df_plot_results], axis=1)\n",
    "\n",
    "# convert from exceedence probability to return period:\n",
    "plot_data.iloc[:, :6] = np.where(plot_data.iloc[:, :6] == 0, 50000, 1/plot_data.iloc[:, :6])\n",
    "plot_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6d058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Plot the data: \n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib  as mpl\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "\n",
    "# Define the colors\n",
    "colors = [(0.275, 0.031, 0.059,1), (0.651,0.153,0.086,1),(0.867, 0.290, 0.141,1),(0.925, 0.518, 0.290,1),\n",
    "          (0.953, 0.698, 0.514,1), (0.973, 0.886, 0.824,1), (0.5, 0.5, 0.5,0.4)]\n",
    "\n",
    "# Define the bins\n",
    "bins = [1, 5, 10, 15, 20, 25, 100, 500000]\n",
    "\n",
    "# Create the colormap\n",
    "cmap = mcolors.ListedColormap(colors)\n",
    "norm = mcolors.BoundaryNorm(bins, cmap.N)\n",
    "\n",
    "# Create the figure\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=0, hspace=0.2)\n",
    "\n",
    "# Create the subplots with 2 rows and 3 columns\n",
    "ax1 = fig.add_subplot(3, 2, 1, projection=ccrs.Robinson())\n",
    "ax2 = fig.add_subplot(3, 2, 2, projection=ccrs.Robinson())\n",
    "ax3 = fig.add_subplot(3, 2, 3, projection=ccrs.Robinson())\n",
    "ax4 = fig.add_subplot(3, 2, 4, projection=ccrs.Robinson())\n",
    "ax5 = fig.add_subplot(3, 2, 5, projection=ccrs.Robinson())\n",
    "ax6 = fig.add_subplot(3, 2, 6, projection=ccrs.Robinson())\n",
    "\n",
    "# add coastlines and country borders\n",
    "ax1.add_feature(cfeature.COASTLINE)\n",
    "ax1.set_global()\n",
    "ax1.set_title('1C World')\n",
    "plot_data = plot_data.sort_values(by='1C', ascending=True)\n",
    "im = ax1.scatter(plot_data['lon'], plot_data['lat'], c=plot_data['1C'], cmap=cmap, transform=ccrs.PlateCarree(),\n",
    "           s=5, norm=norm)\n",
    "\n",
    "# add coastlines and country borders\n",
    "ax2.add_feature(cfeature.COASTLINE)\n",
    "ax2.set_global()\n",
    "ax2.set_title('1.5C World')\n",
    "plot_data = plot_data.sort_values(by='1.5C', ascending=True)\n",
    "ax2.scatter(plot_data['lon'], plot_data['lat'], c=plot_data['1.5C'], cmap=cmap, transform=ccrs.PlateCarree(),\n",
    "           s=5,norm=norm)\n",
    "\n",
    "# add coastlines and country borders\n",
    "ax3.add_feature(cfeature.COASTLINE)\n",
    "ax3.set_global()\n",
    "ax3.set_title('2C World')\n",
    "plot_data = plot_data.sort_values(by='2C', ascending=False)\n",
    "ax3.scatter(plot_data['lon'], plot_data['lat'], c=plot_data['2C'], cmap=cmap, transform=ccrs.PlateCarree(),\n",
    "           s=5,norm=norm)\n",
    "\n",
    "# add coastlines and country borders\n",
    "ax4.add_feature(cfeature.COASTLINE)\n",
    "ax4.set_global()\n",
    "ax4.set_title('2.5C World')\n",
    "plot_data = plot_data.sort_values(by='2.5C', ascending=False)\n",
    "ax4.scatter(plot_data['lon'], plot_data['lat'], c=plot_data['2.5C'], cmap=cmap, transform=ccrs.PlateCarree(),\n",
    "           s=5,norm=norm)\n",
    "\n",
    "# add coastlines and country borders\n",
    "ax5.add_feature(cfeature.COASTLINE)\n",
    "ax5.set_global()\n",
    "ax5.set_title('3C World')\n",
    "plot_data = plot_data.sort_values(by='3C', ascending=False)\n",
    "ax5.scatter(plot_data['lon'], plot_data['lat'], c=plot_data['3C'], cmap=cmap, transform=ccrs.PlateCarree(),\n",
    "           s=5,norm=norm)\n",
    "\n",
    "# add coastlines and country borders\n",
    "ax6.add_feature(cfeature.COASTLINE)\n",
    "ax6.set_global()\n",
    "ax6.set_title('3.5C World')\n",
    "plot_data = plot_data.sort_values(by='3.5C', ascending=False)\n",
    "ax6.scatter(plot_data['lon'], plot_data['lat'], c=plot_data['3.5C'], cmap=cmap, transform=ccrs.PlateCarree(),\n",
    "           s=5, norm=norm)\n",
    "\n",
    "# Create the colormap\n",
    "cax = fig.add_axes([0.21, 0.01, 0.6, 0.04])\n",
    "cb = plt.colorbar(im, cax=cax, orientation='horizontal',cmap=cmap,norm=norm)\n",
    "cb.set_ticklabels(['1', '5','10','15','20','25','100','0'])\n",
    "\n",
    "fig.text(0.515, 0.06, 'Non-compensable heat return period', ha='center', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0704bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 5: Calculate statistics for exhibit 5:\n",
    "\n",
    "### Finally, we need to compute some statistics for the results section. Specifically, the number of stations \n",
    "### in Europe that have at least a 1-in-100 year risk of lethal heat at 1, 1.5, 2, and 3, and the same for \n",
    "### the United States.\n",
    "\n",
    "## Get Lat/Lon boundaries for Europe (lon 20-40, lat 60-30):\n",
    "## Get Lat/Lon boundaries for United States (lon 125-60, lat 50-25):\n",
    "## Filter dataframe into three based on if stations are within Global, US, and Europe:\n",
    "\n",
    "# Create the boolean masks for latitude and longitude conditions\n",
    "lat_mask_EU = (plot_data['lat'] >= 30) & (plot_data['lat'] <= 60)\n",
    "lon_mask_EU = (plot_data['lon'] >= 20) & (plot_data['lon'] <= 40)\n",
    "lat_mask_US = (plot_data['lat'] >= 30) & (plot_data['lat'] <= 50)\n",
    "lon_mask_US = (plot_data['lon'] >= 60) & (plot_data['lon'] <= 125)\n",
    "\n",
    "# Apply the masks to filter the DataFrame\n",
    "plot_data_EU = plot_data[lat_mask_EU & lon_mask_EU]\n",
    "plot_data_US = plot_data[lat_mask_US & lon_mask_US]\n",
    "\n",
    "## For each count number of stations with return >100 AND warming level of X,Y,Z, print in order w/ description.\n",
    "\n",
    "# Calculate the percentage of values less than 100 Globally\n",
    "percentage_1 = (plot_data[plot_data.columns[0]] < 100).mean() * 100\n",
    "percentage_15 = (plot_data[plot_data.columns[1]] < 100).mean() * 100\n",
    "percentage_2 = (plot_data[plot_data.columns[2]] < 100).mean() * 100\n",
    "percentage_25 = (plot_data[plot_data.columns[3]] < 100).mean() * 100\n",
    "percentage_3 = (plot_data[plot_data.columns[4]] < 100).mean() * 100\n",
    "percentage_35 = (plot_data[plot_data.columns[5]] < 100).mean() * 100\n",
    "\n",
    "# Calculate the percentage of values less than 100 in Europe\n",
    "percentage_1_EU = (plot_data_EU[plot_data_EU.columns[0]] < 100).mean() * 100\n",
    "percentage_15_EU = (plot_data_EU[plot_data_EU.columns[1]] < 100).mean() * 100\n",
    "percentage_2_EU = (plot_data_EU[plot_data_EU.columns[2]] < 100).mean() * 100\n",
    "percentage_25_EU = (plot_data_EU[plot_data_EU.columns[3]] < 100).mean() * 100\n",
    "percentage_3_EU = (plot_data_EU[plot_data_EU.columns[4]] < 100).mean() * 100\n",
    "percentage_35_EU = (plot_data_EU[plot_data_EU.columns[5]] < 100).mean() * 100\n",
    "\n",
    "# Calculate the percentage of values less than 100 in the US\n",
    "percentage_1_US = (plot_data_US[plot_data_US.columns[0]] < 100).mean() * 100\n",
    "percentage_15_US = (plot_data_US[plot_data_US.columns[1]] < 100).mean() * 100\n",
    "percentage_2_US = (plot_data_US[plot_data_US.columns[2]] < 100).mean() * 100\n",
    "percentage_25_US = (plot_data_US[plot_data_US.columns[3]] < 100).mean() * 100\n",
    "percentage_3_US = (plot_data_US[plot_data_US.columns[4]] < 100).mean() * 100\n",
    "percentage_35_US = (plot_data_US[plot_data_US.columns[5]] < 100).mean() * 100\n",
    "\n",
    "# Calculate the percent of values less than threshold in each column of a dataframe\n",
    "def calculate_percent_less_than(df, threshold):\n",
    "    return df.apply(lambda col: (col < threshold).mean())\n",
    "\n",
    "# Set up the threshold values\n",
    "thresholds = [10, 50, 100]\n",
    "\n",
    "# Create a dictionary to store the resulting dataframes\n",
    "result_dfs = {}\n",
    "\n",
    "# Iterate over the original dataframes\n",
    "for df_name, df in [(\"plot_data\", plot_data), (\"plot_data_EU\", plot_data_EU), (\"plot_data_US\", plot_data_US)]:\n",
    "    # Create a list to store the multiindex rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate over the threshold values\n",
    "    for threshold in thresholds:\n",
    "        # Calculate the percent less than threshold for each column\n",
    "        percent_less_than = calculate_percent_less_than(df, threshold)\n",
    "\n",
    "        # Create a multiindex row with the dataframe name and threshold\n",
    "        row = pd.MultiIndex.from_tuples([(df_name, f\"1-in-{threshold}\")])\n",
    "\n",
    "        # Append the percent less than values as a row to the list\n",
    "        rows.append(pd.DataFrame(percent_less_than).T.set_index(row))\n",
    "\n",
    "    # Concatenate the rows into a single dataframe\n",
    "    result_df = pd.concat(rows)\n",
    "\n",
    "    # Store the result dataframe in the dictionary\n",
    "    result_dfs[df_name] = result_df\n",
    "\n",
    "# Access each resulting dataframe separately\n",
    "result_df_plot_data = result_dfs[\"plot_data\"]\n",
    "result_df_plot_data_EU = result_dfs[\"plot_data_EU\"]\n",
    "result_df_plot_data_US = result_dfs[\"plot_data_US\"]\n",
    "\n",
    "# Display the resulting dataframes separately\n",
    "result_df_plot_data = result_df_plot_data.drop(['lat','lon'],axis=1)\n",
    "result_df_plot_data_EU = result_df_plot_data_EU.drop(['lat','lon'],axis=1)\n",
    "result_df_plot_data_US = result_df_plot_data_US.drop(['lat','lon'],axis=1)\n",
    "\n",
    "percentages_plot = pd.concat([result_df_plot_data, result_df_plot_data_EU, result_df_plot_data_US], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69aaf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 6: Prepare statistics for plotting: \n",
    "\n",
    "# Reset the index to make the multiindex levels as columns\n",
    "df_reset = percentages_plot.reset_index()\n",
    "\n",
    "# Group the dataframe by the top level of the multiindex\n",
    "grouped = df_reset.groupby('level_0')\n",
    "\n",
    "# Create separate dataframes based on the groups\n",
    "for group_name, group_df in grouped:\n",
    "    # Drop the 'level_0' column\n",
    "    group_df = group_df.drop('level_0', axis=1)\n",
    "    \n",
    "    # Set the group name as the dataframe name\n",
    "    globals()[group_name] = group_df.copy()\n",
    "\n",
    "# Set 'column_name' as the index\n",
    "plot_data.set_index('level_1', inplace=True)\n",
    "plot_data_EU.set_index('level_1', inplace=True)\n",
    "plot_data_US.set_index('level_1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cae9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 7: Plot exhibit 5:\n",
    "\n",
    "y = np.linspace(0, 1, num=100)  # Adjust the 'num' parameter to change the number of data points\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].scatter(plot_data.iloc[1],plot_data.columns, marker='x', color='red',s=60)\n",
    "axes[0].scatter(plot_data.iloc[2],plot_data.columns, marker='o', color='red',s=60)\n",
    "axes[0].scatter(plot_data.iloc[0],plot_data.columns, marker='+', color='red',s=60)\n",
    "axes[0].set_xlim(0,0.65)\n",
    "axes[0].set_title(\"Global\")\n",
    "axes[0].set_ylabel(\"Global Avg. Temp. Increase\")\n",
    "axes[0].text(0.04,4.6, \"A\",fontsize=18)\n",
    "\n",
    "axes[1].scatter(plot_data_US.iloc[1],plot_data.columns, marker='x', color='blue',s=60)\n",
    "axes[1].scatter(plot_data_US.iloc[2],plot_data.columns, marker='o', color='blue',s=60)\n",
    "axes[1].scatter(plot_data_US.iloc[0],plot_data.columns, marker='+', color='blue',s=60)\n",
    "axes[1].set_xlim(0,0.65)\n",
    "axes[1].set_title(\"United States\")\n",
    "axes[1].text(0.04,4.6, \"B\",fontsize=18)\n",
    "axes[1].set_xlabel(\"Percent of Stations Analyzed\")\n",
    "axes[1].set_yticks([])\n",
    "axes[1].set_yticklabels([])\n",
    "\n",
    "axes[2].scatter(plot_data_EU.iloc[1],plot_data.columns, marker='x', color='green',s=60)\n",
    "axes[2].scatter(plot_data_EU.iloc[2],plot_data.columns, marker='o', color='green',s=60)\n",
    "axes[2].scatter(plot_data_EU.iloc[0],plot_data.columns, marker='+', color='green',s=60)\n",
    "axes[2].set_xlim(0,0.65)\n",
    "axes[2].set_title(\"Europe\")\n",
    "axes[2].text(0.04,4.6, \"C\",fontsize=18)\n",
    "axes[2].set_yticks([])\n",
    "axes[2].set_yticklabels([])\n",
    "\n",
    "# Create custom legend symbols and labels\n",
    "symbols = ['+', 'x', 'o']\n",
    "labels = ['1-in-10', '1-in-50', '1-in-100']\n",
    "\n",
    "# Create custom legend handles\n",
    "handles = [plt.Line2D([], [], color='black', marker=symbol, linestyle='None') for symbol in symbols]\n",
    "\n",
    "# Add the legend\n",
    "fig.legend(handles, labels, loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GEV_fit]",
   "language": "python",
   "name": "conda-env-GEV_fit-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
